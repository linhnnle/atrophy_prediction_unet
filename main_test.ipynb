{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SegmentationDataSet2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransform\u001b[39;00m \u001b[39mimport\u001b[39;00m resize\n\u001b[0;32m---> 10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mSegmentationDataSet2\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformations\u001b[39;00m \u001b[39mimport\u001b[39;00m ComposeDouble, AlbuSeg2d, FunctionWrapperDouble, normalize_01, create_dense_target\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munet\u001b[39;00m \u001b[39mimport\u001b[39;00m UNet\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'SegmentationDataSet2'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "import albumentations\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.transform import resize\n",
    "import SegmentationDataSet2\n",
    "from transformations import ComposeDouble, AlbuSeg2d, FunctionWrapperDouble, normalize_01, create_dense_target\n",
    "from unet import UNet\n",
    "from trainer import Trainer\n",
    "\n",
    "\n",
    "# root directory\n",
    "root = pathlib.Path.cwd() / 'Carvana'\n",
    "\n",
    "\n",
    "def get_filenames_of_path(path: pathlib.Path, ext: str = '*'):\n",
    "    \"\"\"Returns a list of files in a directory/path. Uses pathlib.\"\"\"\n",
    "    filenames = [file for file in path.glob(ext) if file.is_file()]\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# input and target files\n",
    "inputs = get_filenames_of_path(root / 'Input')\n",
    "targets = get_filenames_of_path(root / 'Target')\n",
    "\n",
    "# pre-transformations\n",
    "pre_transforms = ComposeDouble([\n",
    "    FunctionWrapperDouble(resize,\n",
    "                          input=True,\n",
    "                          target=False,\n",
    "                          output_shape=(128, 128, 3)),\n",
    "    FunctionWrapperDouble(resize,\n",
    "                          input=False,\n",
    "                          target=True,\n",
    "                          output_shape=(128, 128),\n",
    "                          order=0,\n",
    "                          anti_aliasing=False,\n",
    "                          preserve_range=True),\n",
    "])\n",
    "\n",
    "# training transformations and augmentations\n",
    "transforms_training = ComposeDouble([\n",
    "    AlbuSeg2d(albumentations.HorizontalFlip(p=0.5)),\n",
    "    FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "    FunctionWrapperDouble(np.moveaxis, input=True, target=False, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01)\n",
    "])\n",
    "\n",
    "# validation transformations\n",
    "transforms_validation = ComposeDouble([\n",
    "    FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "    FunctionWrapperDouble(np.moveaxis, input=True, target=False, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01)\n",
    "])\n",
    "\n",
    "# random seed\n",
    "random_seed = 42\n",
    "\n",
    "# split dataset into training set and validation set\n",
    "train_size = 0.8  # 80:20 split\n",
    "\n",
    "inputs_train, inputs_valid = train_test_split(\n",
    "    inputs,\n",
    "    random_state=random_seed,\n",
    "    train_size=train_size,\n",
    "    shuffle=True)\n",
    "\n",
    "targets_train, targets_valid = train_test_split(\n",
    "    targets,\n",
    "    random_state=random_seed,\n",
    "    train_size=train_size,\n",
    "    shuffle=True)\n",
    "\n",
    "# inputs_train, inputs_valid = inputs[:80], inputs[80:]\n",
    "# targets_train, targets_valid = targets[:80], targets[:80]\n",
    "\n",
    "# dataset training\n",
    "dataset_train = SegmentationDataSet2(inputs=inputs_train,\n",
    "                                    targets=targets_train,\n",
    "                                    transform=transforms_training,\n",
    "                                    use_cache=True,\n",
    "                                    pre_transform=pre_transforms)\n",
    "\n",
    "# dataset validation\n",
    "dataset_valid = SegmentationDataSet2(inputs=inputs_valid,\n",
    "                                    targets=targets_valid,\n",
    "                                    transform=transforms_validation,\n",
    "                                    use_cache=True,\n",
    "                                    pre_transform=pre_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual import DatasetViewer\n",
    "dataset_viewer_training = DatasetViewer(dataset_train)\n",
    "dataset_viewer_training.napari()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae4d5529164bcdede1d7a7588c4dd4bbd6dc221689994c7461f870c59218bbe1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
